{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌸 Classificação de Flores Iris com Wang-Mendel\n",
    "\n",
    "**Aula 3 - Minicurso de Sistemas de Inferência Fuzzy**\n",
    "\n",
    "---\n",
    "\n",
    "## 📋 Objetivo\n",
    "\n",
    "Neste notebook, vamos aplicar o **Método de Wang-Mendel** para classificar flores Iris usando apenas **2 variáveis**:\n",
    "- **Petal Length** (comprimento da pétala)\n",
    "- **Petal Width** (largura da pétala)\n",
    "\n",
    "### Por que Iris?\n",
    "- Dataset clássico de Machine Learning (Fisher, 1936)\n",
    "- 3 espécies: *Setosa*, *Versicolor*, *Virginica*\n",
    "- 150 amostras (50 de cada espécie)\n",
    "- **Vantagem do Wang-Mendel**: Regras interpretáveis!\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Referências\n",
    "- Wang, L. X., & Mendel, J. M. (1992). \"Generating fuzzy rules by learning from examples.\"\n",
    "- Fisher, R. A. (1936). \"The use of multiple measurements in taxonomic problems.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Instalação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyfuzzy-toolbox[ml] scikit-learn -q\n",
    "\n",
    "print('✅ pyfuzzy-toolbox e scikit-learn instalados com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import fuzzy_systems as fs\n",
    "from fuzzy_systems.learning import WangMendelLearning\n",
    "from fuzzy_systems.inference import MamdaniSystem\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuração\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "print('✅ Bibliotecas importadas!')\n",
    "print(f'   pyfuzzy-toolbox: {fs.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Passo 1: Carregar e Explorar Dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data  # Shape (150, 4) - 4 features\n",
    "y = iris.target\n",
    "\n",
    "feature_names = iris.feature_names \n",
    "class_names = iris.target_names\n",
    "\n",
    "# feature_names = ['Petal Length (cm)', 'Petal Width (cm)']\n",
    "# class_names = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "\n",
    "# Estatísticas\n",
    "print('📈 Estatísticas:')\n",
    "for i, name in enumerate(feature_names):\n",
    "    print(f'   {name:20s}: [{X[:, i].min():.2f}, {X[:, i].max():.2f}]')\n",
    "print()\n",
    "\n",
    "# Distribuição de classes\n",
    "print('🌸 Distribuição de classes:')\n",
    "for i, name in enumerate(class_names):\n",
    "    count = np.sum(y == i)\n",
    "    print(f'   {name:12s}: {count} amostras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot colorido por classe\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "colors = ['red', 'green', 'blue']\n",
    "markers = ['o', 's', '^']\n",
    "\n",
    "for i, (name, color, marker) in enumerate(zip(class_names, colors, markers)):\n",
    "    idx = y == i\n",
    "    plt.scatter(X[idx, 2], X[idx, 3], \n",
    "                c=color, marker=marker, s=100, \n",
    "                label=name, alpha=0.7, edgecolors='black')\n",
    "\n",
    "plt.xlabel('Petal Length (cm)', fontsize=12)\n",
    "plt.ylabel('Petal Width (cm)', fontsize=12)\n",
    "plt.title('Dataset Iris - Visualização 2D', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('✅ As classes são visualmente separáveis!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. LOAD AND PREPARE DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🌸 WANG-MENDEL CLASSIFICATION - IRIS DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "print(f\"\\n📊 Dataset Information:\")\n",
    "print(f\"   • Samples: {X.shape[0]}\")\n",
    "print(f\"   • Features: {X.shape[1]} ({', '.join(feature_names)})\")\n",
    "print(f\"   • Classes: {len(class_names)} ({', '.join(class_names)})\")\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "print(f\"\\n   • Target shape: {y.shape} → One-hot: {y_onehot.shape}\")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train_onehot, y_test_onehot = train_test_split(\n",
    "    X, y_onehot, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "y_train = np.argmax(y_train_onehot, axis=1)\n",
    "y_test = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "print(f\"\\n   • Train samples: {X_train.shape[0]}\")\n",
    "print(f\"   • Test samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. CREATE FUZZY SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"🔧 CREATING FUZZY SYSTEM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create Mamdani system\n",
    "sistema = MamdaniSystem(name='IrisClassifier')\n",
    "\n",
    "# Add input variables with 3 fuzzy partitions each\n",
    "n_partitions = 3\n",
    "partition_names = ['low', 'medium', 'high']\n",
    "\n",
    "for i, feature_name in enumerate(feature_names):\n",
    "    # Get feature range\n",
    "    x_min = float(X_train[:, i].min())\n",
    "    x_max = float(X_train[:, i].max())\n",
    "    margin = (x_max - x_min) * 0.05\n",
    "    \n",
    "    # Add input variable\n",
    "    sistema.add_input(feature_name, (x_min - margin, x_max + margin))\n",
    "    \n",
    "    # Add fuzzy terms\n",
    "    x_range = (x_max + margin) - (x_min - margin)\n",
    "    step = x_range / (n_partitions - 1)\n",
    "    \n",
    "    for j, term_name in enumerate(partition_names):\n",
    "        center = (x_min - margin) + j * step\n",
    "        left = max(x_min - margin, center - step)\n",
    "        right = min(x_max + margin, center + step)\n",
    "        \n",
    "        if j == 0:\n",
    "            params = [x_min - margin, x_min - margin, center + step]\n",
    "        elif j == n_partitions - 1:\n",
    "            params = [center - step, x_max + margin, x_max + margin]\n",
    "        else:\n",
    "            params = [left, center, right]\n",
    "        \n",
    "        sistema.add_term(feature_name, term_name, 'triangular', params)\n",
    "    \n",
    "    print(f\"   ✓ {feature_name}: {n_partitions} terms\")\n",
    "\n",
    "# Add output variables (one per class, binary: no/yes)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    sistema.add_output(class_name, (0, 1))\n",
    "    sistema.add_term(class_name, 'no', 'triangular', [0, 0, 1.0])\n",
    "    sistema.add_term(class_name, 'yes', 'triangular', [0, 1, 1])\n",
    "    print(f\"   ✓ Output '{class_name}': binary (no/yes)\")\n",
    "\n",
    "print(f\"\\n   Total variables: {len(sistema.input_variables)} inputs, {len(sistema.output_variables)} outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. TRAIN WITH WANG-MENDEL\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"🤖 TRAINING WITH WANG-MENDEL ALGORITHM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create Wang-Mendel learner with scaling enabled\n",
    "wm = WangMendelLearning(\n",
    "    sistema, \n",
    "    X_train, \n",
    "    y_train_onehot,\n",
    "    task='auto',  # Will auto-detect classification\n",
    "    scale_classification=True,\n",
    "    verbose_init=True\n",
    ")\n",
    "\n",
    "# Train\n",
    "sistema_treinado = wm.fit(verbose=True)\n",
    "\n",
    "# Get training statistics\n",
    "stats = wm.get_training_stats()\n",
    "print(f\"\\n📈 Training Statistics:\")\n",
    "print(f\"   • Task type: {stats['task']}\")\n",
    "print(f\"   • Candidate rules: {stats['candidate_rules']}\")\n",
    "print(f\"   • Final rules: {stats['final_rules']}\")\n",
    "print(f\"   • Conflicts resolved: {stats['conflicts_resolved']}\")\n",
    "print(f\"   • Rule coverage: {stats['final_rules']}/{n_partitions**4} possible combinations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. MAKE PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"🎯 MAKING PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = wm.predict(X_test)\n",
    "y_proba = wm.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_train = accuracy_score(y_train, wm.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n📊 Performance Metrics:\")\n",
    "print(f\"   • Training accuracy: {accuracy_train:.4f} ({accuracy_train*100:.2f}%)\")\n",
    "print(f\"   • Test accuracy: {accuracy_test:.4f} ({accuracy_test*100:.2f}%)\")\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\n📋 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"📊 GENERATING VISUALIZATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# ===== Confusion Matrix (SEM SEABORN) =====\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot com imshow\n",
    "im = ax1.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "cbar = fig.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "cbar.set_label('Count', rotation=270, labelpad=15)\n",
    "\n",
    "# Configurar eixos\n",
    "ax1.set(xticks=np.arange(len(class_names)),\n",
    "        yticks=np.arange(len(class_names)),\n",
    "        xticklabels=class_names, \n",
    "        yticklabels=class_names,\n",
    "        ylabel='True Label',\n",
    "        xlabel='Predicted Label',\n",
    "        title='Confusion Matrix')\n",
    "\n",
    "# Adicionar valores\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        text_color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
    "        ax1.text(j, i, int(cm[i, j]),\n",
    "                ha=\"center\", va=\"center\", \n",
    "                color=text_color, fontsize=12, fontweight='bold')\n",
    "\n",
    "# ===== Probability Distribution =====\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "for i, class_name in enumerate(class_names):\n",
    "    ax2.hist(y_proba[:, i], bins=20, alpha=0.6, label=class_name, edgecolor='black')\n",
    "ax2.set_xlabel('Probability', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Predicted Probability Distribution', fontsize=13, fontweight='bold', pad=10)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== Prediction Confidence =====\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "max_proba = y_proba.max(axis=1)\n",
    "colors = ['green' if p == t else 'red' for p, t in zip(y_pred, y_test)]\n",
    "ax3.scatter(range(len(y_test)), max_proba, c=colors, alpha=0.6, edgecolors='black')\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', linewidth=1, alpha=0.5)\n",
    "ax3.set_xlabel('Sample Index', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Max Probability', fontsize=11, fontweight='bold')\n",
    "ax3.set_title('Prediction Confidence (Green=Correct, Red=Wrong)', \n",
    "             fontsize=13, fontweight='bold', pad=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== Feature Distributions by Class (True vs Predicted) =====\n",
    "for idx, feature_idx in enumerate([2, 3]):  # Petal length and width (most discriminative)\n",
    "    ax = fig.add_subplot(gs[1, idx])\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        mask_true = (y_test == i)\n",
    "        ax.hist(X_test[mask_true, feature_idx], bins=15, alpha=0.5, \n",
    "               label=f'{class_name} (true)', edgecolor='black')\n",
    "    \n",
    "    ax.set_xlabel(feature_names[feature_idx], fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'Distribution: {feature_names[feature_idx]}', \n",
    "                fontsize=13, fontweight='bold', pad=10)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== Misclassification Analysis =====\n",
    "ax5 = fig.add_subplot(gs[1, 2])\n",
    "errors = (y_pred != y_test)\n",
    "error_indices = np.where(errors)[0]\n",
    "if len(error_indices) > 0:\n",
    "    error_proba = y_proba[error_indices]\n",
    "    x_pos = np.arange(len(error_indices))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i in range(3):\n",
    "        ax5.bar(x_pos + i*width, error_proba[:, i], width, \n",
    "               label=class_names[i], alpha=0.8)\n",
    "    \n",
    "    ax5.set_xlabel('Misclassified Sample', fontsize=11, fontweight='bold')\n",
    "    ax5.set_ylabel('Class Probability', fontsize=11, fontweight='bold')\n",
    "    ax5.set_title(f'Misclassified Samples ({len(error_indices)} total)', \n",
    "                 fontsize=13, fontweight='bold', pad=10)\n",
    "    ax5.set_xticks(x_pos + width)\n",
    "    ax5.set_xticklabels([f'S{i}' for i in error_indices], rotation=45)\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3, axis='y')\n",
    "else:\n",
    "    ax5.text(0.5, 0.5, '✅ Perfect Classification!\\n No errors found', \n",
    "            ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "            transform=ax5.transAxes)\n",
    "    ax5.axis('off')\n",
    "\n",
    "# ===== Pairwise Feature Plot (Most Important Features) =====\n",
    "ax6 = fig.add_subplot(gs[2, :2])\n",
    "feature_x, feature_y = 2, 3  # Petal length vs width\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # True labels\n",
    "    mask_true = (y_test == i)\n",
    "    ax6.scatter(X_test[mask_true, feature_x], X_test[mask_true, feature_y],\n",
    "               s=100, alpha=0.3, label=f'{class_name} (true)', edgecolors='black')\n",
    "    \n",
    "    # Misclassified\n",
    "    mask_error = (y_test == i) & (y_pred != i)\n",
    "    if mask_error.sum() > 0:\n",
    "        ax6.scatter(X_test[mask_error, feature_x], X_test[mask_error, feature_y],\n",
    "                   s=200, marker='x', linewidths=3, color='red', \n",
    "                   label=f'{class_name} (error)')\n",
    "\n",
    "ax6.set_xlabel(feature_names[feature_x], fontsize=11, fontweight='bold')\n",
    "ax6.set_ylabel(feature_names[feature_y], fontsize=11, fontweight='bold')\n",
    "ax6.set_title('Feature Space: True vs Predicted', fontsize=13, fontweight='bold', pad=10)\n",
    "ax6.legend(fontsize=9, loc='upper left')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# ===== Rules Summary =====\n",
    "ax7 = fig.add_subplot(gs[2, 2])\n",
    "ax7.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "WANG-MENDEL IRIS CLASSIFIER\n",
    "{'='*35}\n",
    "\n",
    "📊 DATASET\n",
    "  • Total samples: {len(X)}\n",
    "  • Training: {len(X_train)}\n",
    "  • Testing: {len(X_test)}\n",
    "  • Classes: {len(class_names)}\n",
    "\n",
    "🔧 SYSTEM CONFIGURATION\n",
    "  • Inputs: {len(feature_names)}\n",
    "  • Partitions per input: {n_partitions}\n",
    "  • Total possible rules: {n_partitions**len(feature_names)}\n",
    "  • Generated rules: {stats['final_rules']}\n",
    "\n",
    "🎯 PERFORMANCE\n",
    "  • Train accuracy: {accuracy_train*100:.1f}%\n",
    "  • Test accuracy: {accuracy_test*100:.1f}%\n",
    "  • Errors: {(y_pred != y_test).sum()}/{len(y_test)}\n",
    "\n",
    "⚙️ SCALING\n",
    "  • Output scaling: ENABLED\n",
    "  • Method: Structure-based\n",
    "\"\"\"\n",
    "\n",
    "ax7.text(0.05, 0.95, summary_text, transform=ax7.transAxes,\n",
    "        fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "plt.suptitle('Wang-Mendel Fuzzy Classification - Iris Dataset', \n",
    "            fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Visualization complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. VISUALIZAR REGRAS\n",
    "# ============================================================================\n",
    "wm.system.plot_rule_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter memberships\n",
    "memberships = wm.predict_membership(X_test)\n",
    "\n",
    "# Para classificação Iris (3 classes × 2 termos cada = 'no'/'yes')\n",
    "print(memberships['setosa'].shape)  # (45, 2) - 45 amostras, 2 termos\n",
    "\n",
    "# Primeira amostra\n",
    "print(f\"Setosa - no: {memberships['setosa'][0, 0]:.3f}, yes: {memberships['setosa'][0, 1]:.3f}\")\n",
    "print(f\"Versicolor - no: {memberships['versicolor'][0, 0]:.3f}, yes: {memberships['versicolor'][0, 1]:.3f}\")\n",
    "print(f\"Virginica - no: {memberships['virginica'][0, 0]:.3f}, yes: {memberships['virginica'][0, 1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (generico)",
   "language": "python",
   "name": "generico"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}